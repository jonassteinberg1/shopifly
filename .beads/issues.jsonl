{"id":"shopifly-1hb","title":"CHECKPOINT: Human reviews final output","description":"HUMAN TASK - Final review before merge.\nReview:\n1. All tests pass\n2. Dashboard looks good\n3. Code review git diff\n4. Approve merge to main","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T16:13:14.931577367Z","created_by":"ec2-user","updated_at":"2026-01-10T17:04:35.120445931Z","closed_at":"2026-01-10T17:04:35.120445931Z","close_reason":"Removed - fully autonomous"}
{"id":"shopifly-37t","title":"TDD: Dashboard tests written (must pass before dashboard complete)","description":"# Task: TDD Tests for Dashboard\n\n## Objective\nVerify TDD test specifications exist and define the contract that the dashboard must fulfill.\n\n## Context\nTests at: tests/tdd/test_dashboard_spec.py\nALL commands run through Docker (Python 3.11).\n\n## Execution Steps\n\n### Step 1: Verify tests exist\n```bash\nls -la tests/tdd/test_dashboard_spec.py\n```\n\n### Step 2: Run tests (should FAIL before dashboard built)\n```bash\ndocker compose run --rm test tests/tdd/test_dashboard_spec.py -v\n```\nExpected: FAILED (dashboard directory missing)\n\n### Step 3: Review test specifications\n```bash\ncat tests/tdd/test_dashboard_spec.py\n```\n\n## Acceptance Criteria\n- [ ] tests/tdd/test_dashboard_spec.py exists\n- [ ] Tests are runnable with docker compose run test\n- [ ] Tests define clear contract for dashboard\n\n## Reference\n- TDD Tests: tests/tdd/test_dashboard_spec.py\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-10T16:13:59.127355358Z","created_by":"ec2-user","updated_at":"2026-01-10T19:05:24.461163181Z","closed_at":"2026-01-10T19:05:24.461163181Z","close_reason":"Closed"}
{"id":"shopifly-43j","title":"Build Streamlit dashboard with 6 visualizations","description":"# Task: Build Streamlit Dashboard\n\n## Objective\nCreate a visualization dashboard that displays classified insights with 6 chart types.\n\n## Context\nDepends on: shopifly-nya (classified data must exist)\nALL commands run through Docker (Python 3.11).\n\n## Architecture\n\n```\ndashboard/\n├── __init__.py      # Package init\n├── data.py          # SQLite query functions (6 functions)\n├── charts.py        # Chart generation helpers (5 functions)\n└── app.py           # Main Streamlit app (6 sections)\n```\n\n## Execution Steps\n\n### Step 1: Create dashboard directory (from dev container)\n```bash\ndocker compose run --rm dev -c \"mkdir -p dashboard \u0026\u0026 touch dashboard/__init__.py\"\n```\n\n### Step 2: Create data.py\nQuery functions that fetch data from SQLite:\n```python\n# dashboard/data.py\ndef get_db_path(): ...\ndef get_insights_summary(): ...      # Total counts, avg frustration\ndef get_category_breakdown(): ...    # Count by category\ndef get_trends_data(): ...           # Insights over time\ndef get_keyword_frequencies(): ...   # Top keywords\ndef get_top_opportunities(): ...     # Ranked opportunities\ndef get_competitor_mentions(): ...   # Competitor analysis\n```\n\nReference existing patterns in: storage/sqlite.py\n\n### Step 3: Create charts.py\nChart generation using Plotly:\n```python\n# dashboard/charts.py\ndef create_category_chart(data): ...      # Bar chart\ndef create_trends_chart(data): ...        # Line chart\ndef create_wordcloud(keywords): ...       # Word cloud\ndef create_opportunities_table(data): ... # Sortable table\ndef create_competitor_chart(data): ...    # Bar chart\n```\n\n### Step 4: Create app.py\nMain Streamlit application with 6 sections.\n\n### Step 5: Add streamlit service to docker-compose.yml\n```yaml\n  dashboard:\n    build:\n      context: .\n      dockerfile: Dockerfile\n    volumes:\n      - .:/app\n      - ./data:/app/data\n      - ./.env:/app/.env:ro\n    ports:\n      - \"8501:8501\"\n    environment:\n      - PYTHONUNBUFFERED=1\n    command: [\"streamlit\", \"run\", \"dashboard/app.py\", \"--server.address\", \"0.0.0.0\", \"--server.port\", \"8501\"]\n```\n\n### Step 6: Update Dockerfile to include streamlit\nAdd to Dockerfile if not present:\n```dockerfile\nRUN pip install streamlit plotly wordcloud\n```\n\n### Step 7: Verify TDD tests pass\n```bash\ndocker compose run --rm test tests/tdd/test_dashboard_spec.py -v\n```\n\n## Acceptance Criteria\n- [ ] dashboard/ directory exists with 4 files\n- [ ] data.py has 6 query functions\n- [ ] charts.py has 5 chart functions\n- [ ] app.py has 6 visible sections\n- [ ] TDD tests pass\n- [ ] Each chart renders with real data\n\n## Out of Scope\n- Filters, export, auth, mobile\n\n## Reference\n- PRD: docs/plans/meta-shopifly-orchestrator.md (Track E)\n- ADR: docs/adr/001-streamlit-dashboard.md\n","status":"closed","priority":0,"issue_type":"feature","created_at":"2026-01-10T03:17:42.949469342Z","created_by":"ec2-user","updated_at":"2026-01-10T19:15:07.385873145Z","closed_at":"2026-01-10T19:15:07.385873145Z","close_reason":"Closed","dependencies":[{"issue_id":"shopifly-43j","depends_on_id":"shopifly-nya","type":"blocks","created_at":"2026-01-10T03:17:54.714833754Z","created_by":"ec2-user"},{"issue_id":"shopifly-43j","depends_on_id":"shopifly-37t","type":"blocks","created_at":"2026-01-10T17:04:35.141065617Z","created_by":"ec2-user"}]}
{"id":"shopifly-5gj","title":"Run E2E tests to verify pipeline","description":"# Task: Run E2E Tests - Final Verification\n\n## Objective\nRun all tests including a Selenium browser test that verifies the dashboard is accessible over the internet.\n\n## CRITICAL: Selenium Dashboard Test\n\n### Create test file: tests/e2e/test_dashboard_live.py\n```python\nimport pytest\nfrom selenium import webdriver\nfrom selenium.webdriver.chrome.options import Options\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\nDASHBOARD_URL = \"http://54.197.8.56:8501\"\n\n@pytest.fixture\ndef browser():\n    options = Options()\n    options.add_argument(\"--headless\")\n    options.add_argument(\"--no-sandbox\")\n    options.add_argument(\"--disable-dev-shm-usage\")\n    driver = webdriver.Chrome(options=options)\n    driver.set_page_load_timeout(30)\n    yield driver\n    driver.quit()\n\ndef test_dashboard_loads(browser):\n    \"\"\"Dashboard should load and show title.\"\"\"\n    browser.get(DASHBOARD_URL)\n    WebDriverWait(browser, 20).until(\n        EC.presence_of_element_located((By.TAG_NAME, \"h1\"))\n    )\n    assert \"Shopify\" in browser.title or \"Streamlit\" in browser.title\n\ndef test_dashboard_has_metrics(browser):\n    \"\"\"Dashboard should show metric cards.\"\"\"\n    browser.get(DASHBOARD_URL)\n    WebDriverWait(browser, 20).until(\n        EC.presence_of_element_located((By.CSS_SELECTOR, \"[data-testid=stMetric]\"))\n    )\n    metrics = browser.find_elements(By.CSS_SELECTOR, \"[data-testid=stMetric]\")\n    assert len(metrics) \u003e= 3, f\"Expected 3+ metrics, got {len(metrics)}\"\n\ndef test_dashboard_has_charts(browser):\n    \"\"\"Dashboard should render at least one chart.\"\"\"\n    browser.get(DASHBOARD_URL)\n    WebDriverWait(browser, 20).until(\n        EC.presence_of_element_located((By.CSS_SELECTOR, \".plotly, .js-plotly-plot\"))\n    )\n```\n\n## Execution Steps\n\n### Step 1: Ensure dashboard is running\n```bash\ncurl -s http://localhost:8501 | head -5\n# Must return HTML\n```\n\n### Step 2: Create Selenium test file\nCreate tests/e2e/test_dashboard_live.py with the code above.\n\n### Step 3: Run all tests\n```bash\n# Run Selenium dashboard test\npytest tests/e2e/test_dashboard_live.py -v\n\n# Run other tests\npytest tests/tdd -v\npytest tests/unit -v\npytest tests/e2e -v --ignore=tests/e2e/test_dashboard_live.py\n```\n\n### Step 4: Verify all pass\nAll tests must pass (or have expected skips with documented reasons).\n\n## Acceptance Criteria\n- [ ] tests/e2e/test_dashboard_live.py exists\n- [ ] Selenium test navigates to http://54.197.8.56:8501\n- [ ] test_dashboard_loads passes\n- [ ] test_dashboard_has_metrics passes\n- [ ] test_dashboard_has_charts passes\n- [ ] All other test suites pass\n\n## Definition of DONE\nWhen Selenium can load the dashboard over the internet and verify metrics/charts are visible, the pipeline is COMPLETE.\n\n## Reference\n- Dashboard URL: http://54.197.8.56:8501\n- Selenium requires Chrome + chromedriver (already in Docker image)\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T03:17:42.999672861Z","created_by":"ec2-user","updated_at":"2026-01-11T01:36:49.561358057Z","closed_at":"2026-01-11T01:36:49.561358057Z","close_reason":"Closed","dependencies":[{"issue_id":"shopifly-5gj","depends_on_id":"shopifly-fkp","type":"blocks","created_at":"2026-01-10T03:17:54.759881143Z","created_by":"ec2-user"}]}
{"id":"shopifly-bbu","title":"Scrape data from all sources (reddit 100, appstore 50, community 50)","description":"# Task: Data Collection Pipeline\n\n## Objective\nPopulate the SQLite database with raw merchant feedback from multiple sources for subsequent LLM classification.\n\n## Context\nThe database is currently EMPTY. This is the foundation task - nothing else can proceed until we have data.\nALL commands run through Docker (Python 3.11).\n\n## Execution Steps\n\n### Step 0: Build Docker image (if not already built)\n```bash\ndocker compose build app\n```\n\n### Step 1: Scrape Reddit (100 posts)\n```bash\ndocker compose run --rm app scrape --source reddit --storage sqlite --limit 100\n```\n- Uses RSS-based scraper (NOT JSON API - was denied)\n- Implementation: scrapers/reddit_selenium.py\n- Targets r/shopify and related subreddits\n\n### Step 2: Scrape App Store (50 reviews)\n```bash\ndocker compose run --rm app scrape --source appstore --storage sqlite --limit 50\n```\n- Scrapes Shopify app store reviews\n- Implementation: scrapers/appstore.py\n\n### Step 3: Scrape Community Forums (50 posts)\n```bash\ndocker compose run --rm app scrape --source community --storage sqlite --limit 50\n```\n- Scrapes Shopify Community forums\n- Implementation: scrapers/community.py\n\n### Step 4: Verify\n```bash\ndocker compose run --rm app stats --storage sqlite\n```\n\n## Acceptance Criteria\n- [ ] Raw Data Points \u003e= 150\n- [ ] No scraper errors in output\n- [ ] Data contains: title, content, source, url, timestamp\n- [ ] SQLite database file exists at data/shopifly.db\n\n## Out of Scope\n- Twitter scraping (API issues)\n- Classification (next task)\n- Data deduplication\n\n## Reference\n- PRD: docs/plans/dynamic-chasing-island.md\n- ADR: docs/adr/003-reddit-rss.md\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-10T03:17:42.894572302Z","created_by":"ec2-user","updated_at":"2026-01-10T19:04:40.137099987Z","closed_at":"2026-01-10T19:04:40.137099987Z","close_reason":"Closed"}
{"id":"shopifly-fkp","title":"Start dashboard server on port 8501","description":"# Task: Start Dashboard Server\n\n## Objective\nInstall streamlit and start the dashboard server on port 8501.\n\n## CRITICAL: Streamlit is NOT installed yet!\n\n## Execution Steps\n\n### Step 1: Install streamlit and dependencies\n```bash\npip install streamlit plotly wordcloud\n```\n\n### Step 2: Verify dashboard files exist\n```bash\nls -la dashboard/*.py\n# Should show: __init__.py, app.py, data.py, charts.py\n```\n\n### Step 3: Start Streamlit server\n```bash\ncd /home/ec2-user/shopifly\nnohup streamlit run dashboard/app.py --server.address 0.0.0.0 --server.port 8501 \u003e streamlit.log 2\u003e\u00261 \u0026\n```\n\n### Step 4: Wait for startup\n```bash\nsleep 5\n```\n\n### Step 5: Verify server is running\n```bash\ncurl -s http://localhost:8501 | head -20\n# Must return HTML - if empty, check streamlit.log for errors\n```\n\n### Step 6: Check process\n```bash\npgrep -f streamlit\n# Must return a PID\n```\n\n## Acceptance Criteria\n- [ ] pip install streamlit completes without error\n- [ ] Streamlit process is running (pgrep returns PID)\n- [ ] curl http://localhost:8501 returns HTML\n- [ ] streamlit.log shows no errors\n\n## DO NOT close this task until curl returns HTML!\n","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-10T03:17:42.973420683Z","created_by":"ec2-user","updated_at":"2026-01-11T00:55:49.634488442Z","closed_at":"2026-01-11T00:55:49.634488442Z","close_reason":"Closed","dependencies":[{"issue_id":"shopifly-fkp","depends_on_id":"shopifly-43j","type":"blocks","created_at":"2026-01-10T03:17:54.737616906Z","created_by":"ec2-user"}]}
{"id":"shopifly-h4w","title":"CHECKPOINT: Human reviews dashboard UI","description":"HUMAN TASK - AI must STOP here and wait for approval.\nReview:\n1. Open http://localhost:8501 in browser\n2. Verify all 6 charts render correctly\n3. Check data looks reasonable\n4. Note any UI issues for v2\n5. Approve to continue or request fixes","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-10T16:13:14.909706922Z","created_by":"ec2-user","updated_at":"2026-01-10T17:04:35.098276467Z","closed_at":"2026-01-10T17:04:35.098276467Z","close_reason":"Removed - fully autonomous","dependencies":[{"issue_id":"shopifly-h4w","depends_on_id":"shopifly-43j","type":"blocks","created_at":"2026-01-10T16:13:27.147118724Z","created_by":"ec2-user"}]}
{"id":"shopifly-nya","title":"Classify scraped data with LLM","description":"# Task: LLM Classification Pipeline\n\n## Objective\nProcess all raw scraped data through the LLM classifier to extract structured insights.\n\n## Context\nDepends on shopifly-bbu (scraping). Raw data must exist before classification.\nALL commands run through Docker (Python 3.11).\n\n## How the Classifier Works\n- File: analysis/classifier.py\n- Model: claude-3-haiku-20240307 (configured in .env)\n- Cost: ~$0.002 per item (~$0.30 for 150 items)\n\n## Execution Steps\n\n### Step 1: Verify raw data exists\n```bash\ndocker compose run --rm app stats --storage sqlite\n# Must show Raw Data Points \u003e= 150\n```\n\n### Step 2: Run classification\n```bash\ndocker compose run --rm app classify --storage sqlite --limit 200 --concurrency 5\n```\n\n### Step 3: Verify classification results\n```bash\ndocker compose run --rm app stats --storage sqlite\n# Must show Classified Insights \u003e= 100\n```\n\n### Step 4: Spot-check quality\n```bash\ndocker compose run --rm app opportunities --storage sqlite --top 10\n```\n\n## Acceptance Criteria\n- [ ] Classified Insights \u003e= 100\n- [ ] Each insight has: category, frustration_level, keywords\n- [ ] No classification errors in output\n- [ ] Cost \u003c $2 total\n\n## Out of Scope\n- Enhanced classification\n- Sonnet model usage\n\n## Reference\n- PRD: docs/plans/meta-shopifly-orchestrator.md (Task A2)\n- ADR: docs/adr/004-haiku-classification.md\n","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-10T03:17:42.918046373Z","created_by":"ec2-user","updated_at":"2026-01-10T19:11:10.997782446Z","closed_at":"2026-01-10T19:11:10.997782446Z","close_reason":"Closed","dependencies":[{"issue_id":"shopifly-nya","depends_on_id":"shopifly-bbu","type":"blocks","created_at":"2026-01-10T03:17:54.691314922Z","created_by":"ec2-user"}]}
{"id":"shopifly-t1f","title":"CHECKPOINT: Human reviews scraped data quality","description":"HUMAN TASK - AI must STOP here and wait for approval.\nReview:\n1. python main.py stats --storage sqlite\n2. Spot-check 5-10 raw data points for quality\n3. Verify no garbage/spam data\n4. Approve to continue or request re-scrape","status":"closed","priority":0,"issue_type":"task","created_at":"2026-01-10T16:13:14.88794571Z","created_by":"ec2-user","updated_at":"2026-01-10T17:04:35.073243271Z","closed_at":"2026-01-10T17:04:35.073243271Z","close_reason":"Removed - fully autonomous","dependencies":[{"issue_id":"shopifly-t1f","depends_on_id":"shopifly-nya","type":"blocks","created_at":"2026-01-10T16:13:27.104986194Z","created_by":"ec2-user"}]}
